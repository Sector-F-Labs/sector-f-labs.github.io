<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Sector F Labs</title>
  <link rel="icon" type="image/png" href="logo-nocircle.png">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Sector F Labs is a space for bold experimentation and beautifully simple tools. We break traditional molds to build systems that are powerful, privacy-respecting, and human-centered.">
  <meta property="og:title" content="Sector F Labs">
  <meta property="og:description" content="Sector F Labs is a space for bold experimentation and beautifully simple tools. We break traditional molds to build systems that are powerful, privacy-respecting, and human-centered.">
  <meta property="og:image" content="logo.png">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://sectorflabs.com/">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Sector F Labs">
  <meta name="twitter:description" content="Sector F Labs is a space for bold experimentation and beautifully simple tools.">
  <meta name="twitter:image" content="logo.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lexend:wght@100..900&family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <header>
    <a href="/index.html">
      <img src="/logo-nocircle.png" alt="Sector F Logo">
      <h1>Sector F Labs</h1>
    </a>
  </header>

  <nav>
    <ul>
      <li><a href="/projects/exp-013-service-pipe">exp-013-service-pipe</a></li>
<li><a href="/projects/reservoir">Reservoir</a></li>
<li><a href="/projects/md-chat">md-chat</a></li>
    </ul>
  </nav>

  <main>
    <h1>ðŸš§ Under Construction</h1>
<blockquote>
<p>Reservoir is in active development. It's not ready for production use yet. Expect breaking changes.</p>
</blockquote>
<p><strong>Recent Updates:</strong></p>
<ul>
<li>âœ… Added intelligent request forwarding for unparseable JSON bodies</li>
<li>âœ… Added support for <code>web_search_options</code> in chat requests</li>
<li>âœ… Enhanced multi-provider support (OpenAI, Ollama, Mistral, Gemini)</li>
<li>âœ… Improved model configuration with automatic provider detection</li>
<li>âœ… Fixed deserialization issues for optional request fields</li>
</ul>
<h1>Reservoir</h1>
<h2>What is Reservoir?</h2>
<p>Reservoir is your helpful memory for AI conversations. It sits between your app and any OpenAI-compatible Chat Completions API, making it easier to have rich, ongoing conversations with your favorite language models from multiple providers.</p>
<h3>Why does this matter?</h3>
<p>When you use any <a href="https://platform.openai.com/docs/guides/chat">OpenAI-compatible Chat Completions API</a>, you need to send the full conversation history with every request. For example:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-json">[
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 1 + 1?&quot;},
  {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;2&quot;},
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the answer times 3?&quot;}
]
</code></pre></div>
<p>If you only send the last question, the model won't know what &quot;the answer&quot; refers to. You have to keep track of all previous messages and include them every time.</p>
<p><strong>This can get tricky as conversations grow!</strong></p>
<p>Reservoir acts as a smart proxy: it automatically stores your chat history and inserts the right context into each request. You just talk to any OpenAI-compatible API as usual and Reservoir handles the memory, context, and even finds other relevant messages from your past conversations to help the model give better answers.</p>
<p><strong>Supported Providers:</strong></p>
<ul>
<li>
<p>OpenAI (GPT-4, GPT-4o, GPT-3.5-turbo, etc.)</p>
</li>
<li>
<p>Ollama (llama3.2, gemma3, and any local models)</p>
</li>
<li>
<p>Mistral AI (mistral-large-2402, etc.)</p>
</li>
<li>
<p>Google Gemini (gemini-2.0-flash, etc.)</p>
</li>
<li>
<p>Any OpenAI-compatible API endpoint</p>
</li>
<li>
<p>No more manual history management</p>
</li>
<li>
<p>Automatic context enrichment</p>
</li>
<li>
<p>Your data stays private and local</p>
</li>
</ul>
<h3>Use Reservoir with Multiple Apps</h3>
<p>You can point multiple apps or clients to a single Reservoir instance. This means you can keep context and history across different tools on your computerâ€”like your terminal, a web app, or a chat client. If you want to keep conversations separate, you can use Reservoir's partitioning feature to organize chats by app, project, or any context you choose.</p>
<h2>Why Use Reservoir?</h2>
<ul>
<li><strong>Own your AI history</strong>: All your conversations are stored locally, never in the cloud.</li>
<li><strong>Search and recall</strong>: Instantly find previous chats, ideas, or code snippets from your AI interactions.</li>
<li><strong>Enrich context</strong>: Automatically inject relevant history into new prompts for more coherent, personalized responses.</li>
<li><strong>Visualize conversations</strong>: See how your discussions branch and connect over time.</li>
<li><strong>Stay private</strong>: Your data never leaves your device.</li>
</ul>
<p><img src="./logo_256.png" alt="Screenshot" /></p>
<p>Reservoir lets you have conversations with multiple AI models and providers, all while keeping your data private and local. Every interaction is stored on your device, building a personal knowledge base that never leaves your network. A single thread of conversation can span multiple models without losing context, allowing you to seamlessly switch between different AI providers while maintaining the flow of your discussion.</p>
<p><strong>Advanced Features:</strong></p>
<ul>
<li><strong>Request Forwarding</strong>: Automatically forwards unparseable requests to the appropriate API while logging details for debugging</li>
<li><strong>Web Search Integration</strong>: Pass <code>web_search_options</code> to enable AI models with web search capabilities</li>
<li><strong>Multi-Provider Routing</strong>: Automatically routes requests to the correct provider based on model name</li>
<li><strong>Token Management</strong>: Smart truncation to stay within model limits while preserving context</li>
<li><strong>Flexible Configuration</strong>: Environment variables for custom provider endpoints</li>
</ul>
<h2>Table of Contents</h2>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#conversation-threads-via-synapses">Conversation Threads via Synapses</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2>Overview</h2>
<p>Reservoir intercepts your API calls, enriches them with relevant history, manages token limits, and then forwards them to the actual LLM service.</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-mermaid">sequenceDiagram
    participant App
    participant Reservoir
    participant Neo4j
    participant LLM as OpenAI/Ollama

    App-&gt;&gt;Reservoir: Request (e.g. /v1/chat/completions/$USER/my-application)
    Reservoir-&gt;&gt;Reservoir: Check if last message exceeds token limit (Return error if true)
    Reservoir-&gt;&gt;Reservoir: Tag with Trace ID + Partition
    Reservoir-&gt;&gt;Neo4j: Store original request message(s)

    %% --- Context Enrichment Steps ---
    Reservoir-&gt;&gt;Neo4j: Query for similar &amp; recent messages
    Neo4j--&gt;&gt;Reservoir: Return relevant context messages
    Reservoir-&gt;&gt;Reservoir: Inject context messages into request payload
    %% --- End Enrichment Steps ---

    Reservoir-&gt;&gt;Reservoir: Check total token count &amp; truncate if needed (preserving system/last messages)

    Reservoir-&gt;&gt;LLM: Forward enriched &amp; potentially truncated request
    LLM-&gt;&gt;Reservoir: Return LLM response
    Reservoir-&gt;&gt;Neo4j: Store LLM response message
    Reservoir-&gt;&gt;App: Return LLM response
</code></pre></div>
<p>This sequence diagram provides a high-level overview of how Reservoir processes requests and responses.</p>
<h2>Conversation Threads via Synapses</h2>
<p>Reservoir uses synapse relationships to create &quot;threads&quot; of semantically related messages within the conversation graph. As messages are added, synapses link them sequentially, forming a continuous flow. When the similarity between messages drops below a threshold, the thread is split, marking a topic change. This results in distinct conversation threads, making it easy to visualize and retrieve related exchanges.</p>
<p>You can see an example of this structure in the following graph visualization:</p>
<p><img src="./conversation_graph_view.png" alt="Conversation Graph View" /></p>
<h2>Documentation</h2>
<p>Reservoir's documentation is organized into the following sections:</p>
<ul>
<li><a href="./docs/architecture.md">Architecture</a>: System and component overview.</li>
<li><a href="./docs/api.md">API</a>: API endpoints, usage, and examples.</li>
<li><a href="./docs/data_model.md">Data Model</a>: How data is stored in Neo4j, including the schema.</li>
<li><a href="./docs/dev.md">Development</a>: Setting up the development environment, running locally, and contributing.</li>
<li><a href="./docs/features.md">Features</a>: Key features and future roadmap.</li>
<li><a href="./docs/deployment.md">Deployment</a>: Steps to deploy Reservoir locally or in production.</li>
<li><a href="./docs/faq.md">FAQ</a>: Troubleshooting, common questions, and tips.</li>
<li><a href="./docs/request_forwarding.md">Request Forwarding</a>: How Reservoir handles unparseable requests.</li>
</ul>
<h2>Quick Start</h2>
<p>Reservoir provides an OpenAI-compatible API endpoint. You can use your system username as the partition and your application name as the instance for best results.</p>
<h3>Starting the Server</h3>
<p>To start the Reservoir server:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">cargo run -- start
</code></pre></div>
<p>This command:</p>
<ol>
<li>Initializes the vector index in Neo4j for semantic search</li>
<li>Starts the server on the configured port (default: 3017)</li>
</ol>
<p>The server will be available at <code>http://localhost:3017</code> (or your configured port).</p>
<h4>Ollama Mode</h4>
<p>For seamless integration with Ollama-compatible clients, you can start Reservoir in Ollama mode:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">cargo run -- start --ollama
</code></pre></div>
<p>This mode configures Reservoir to act as a drop-in replacement for Ollama, making it easy to add memory and context to existing Ollama-based applications.</p>
<h4>Testing Your Setup</h4>
<p>You can test your Reservoir installation using the included hurl tests:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash"># Test all endpoints including request forwarding
./hurl/test.sh

# Test specific endpoints
hurl --variable USER=&quot;$USER&quot; --variable OPENAI_API_KEY=&quot;$OPENAI_API_KEY&quot; hurl/chat_completion.hurl
hurl --variable USER=&quot;$USER&quot; hurl/reservoir-view.hurl
hurl --variable USER=&quot;$USER&quot; hurl/reservoir-search.hurl

# Test request forwarding functionality
hurl --variable USER=&quot;$USER&quot; hurl/test_forwarding.hurl
./test_forwarding.sh
</code></pre></div>
<p>Or test directly with Ollama (if you have it running):</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">hurl hurl/ollama_mode.hurl
</code></pre></div>
<h3>Import/Export Data</h3>
<p>Reservoir supports exporting all message nodes to a JSON file and importing them back into the database. This is useful for backup, migration, or sharing your AI conversation history.</p>
<h4>Export all message nodes to JSON</h4>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">cargo run -- export &gt; messages.json
</code></pre></div>
<p>This command prints all message nodes in the database as pretty-printed JSON to stdout. Redirect the output to a file to save it.</p>
<h4>Import message nodes from a JSON file</h4>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">cargo run -- import path/to/messages.json
</code></pre></div>
<h3>View the last N messages</h3>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">cargo run -- view &lt;COUNT&gt; [--partition &lt;PARTITION&gt;] [--instance &lt;INSTANCE&gt;]
</code></pre></div>
<p>Displays the last <code>&lt;COUNT&gt;</code> messages in the specified partition and instance. If not provided, <code>partition</code> defaults to &quot;default&quot; and <code>instance</code> defaults to the partition.</p>
<p>Example:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">cargo run -- view 5 --partition sales --instance eu-west
</code></pre></div>
<p>Sample output:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code>2025-05-09T14:23:01+00:00 [abc123] user: Hello there!
2025-05-09T14:23:02+00:00 [abc123] assistant: Hi! How can I help?
2025-05-09T14:24:10+00:00 [def456] user: Show me last week's sales report.
2025-05-09T14:24:12+00:00 [def456] assistant: Here is the summary for last week's sales...
2025-05-09T14:25:00+00:00 [ghi789] user: Thanks!
</code></pre></div>
<p>This command reads the specified JSON file (in the same format as the export) and imports all message nodes into the database.</p>
<h3>Example Usage</h3>
<ul>
<li>
<p><strong>Instead of:</strong></p>
<p><code>https://api.openai.com/v1/chat/completions</code></p>
</li>
<li>
<p><strong>Use:</strong></p>
<p><code>http://127.0.0.1:3017/partition/$USER/instance/reservoir/v1/chat/completions</code></p>
</li>
</ul>
<blockquote>
<p>Here, <code>$USER</code> is your system username, and <code>reservoir</code> is the instance name.</p>
</blockquote>
<h4>Curl Examples</h4>
<p><strong>OpenAI Model:</strong></p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">curl &quot;http://127.0.0.1:3017/partition/$USER/instance/reservoir/v1/chat/completions&quot; \
    -H &quot;Content-Type: application/json&quot; \
    -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
    -d '{
        &quot;model&quot;: &quot;gpt-4&quot;,
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Write a one-sentence bedtime story about a brave little toaster.&quot;
            }
        ]
    }'
</code></pre></div>
<p><strong>Ollama Model (no API key needed):</strong></p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">curl &quot;http://127.0.0.1:3017/partition/$USER/instance/reservoir/v1/chat/completions&quot; \
    -H &quot;Content-Type: application/json&quot; \
    -d '{
        &quot;model&quot;: &quot;gemma3&quot;,
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;Explain quantum computing in simple terms.&quot;
            }
        ]
    }'
</code></pre></div>
<p><strong>With Web Search Options:</strong></p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">curl &quot;http://127.0.0.1:3017/partition/$USER/instance/reservoir/v1/chat/completions&quot; \
    -H &quot;Content-Type: application/json&quot; \
    -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
    -d '{
        &quot;model&quot;: &quot;gpt-4o-search-preview&quot;,
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;What are the latest developments in AI?&quot;
            }
        ],
        &quot;web_search_options&quot;: {
            &quot;enabled&quot;: true,
            &quot;max_results&quot;: 5
        }
    }'
</code></pre></div>
<h4>Python Examples (using <code>openai</code> library)</h4>
<p><strong>Basic Usage with OpenAI:</strong></p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-python">import os
from openai import OpenAI

INSTANCE = &quot;my-application&quot;
PARTITION = os.getenv(&quot;USER&quot;)
RESERVOIR_PORT = os.getenv('RESERVOIR_PORT', '3017')
RESERVOIR_BASE_URL = f&quot;http://localhost:{RESERVOIR_PORT}/v1/partition/{PARTITION}/instance/{INSTANCE}&quot;

client = OpenAI(
    base_url=RESERVOIR_BASE_URL,
    api_key=os.environ.get(&quot;OPENAI_API_KEY&quot;)
)

completion = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Write a one-sentence bedtime story about a curious robot.&quot;
        }
    ]
)
print(completion.choices[0].message.content)
</code></pre></div>
<p><strong>Using Ollama (no API key required):</strong></p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-python">import os
from openai import OpenAI

INSTANCE = &quot;my-application&quot;
PARTITION = os.getenv(&quot;USER&quot;)
RESERVOIR_PORT = os.getenv('RESERVOIR_PORT', '3017')
RESERVOIR_BASE_URL = f&quot;http://localhost:{RESERVOIR_PORT}/v1/partition/{PARTITION}/instance/{INSTANCE}&quot;

client = OpenAI(
    base_url=RESERVOIR_BASE_URL,
    api_key=&quot;not-needed-for-ollama&quot;  # Ollama doesn't require API keys
)

completion = client.chat.completions.create(
    model=&quot;llama3.2&quot;,  # or &quot;gemma3&quot;, or any Ollama model
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Explain the concept of recursion with a simple example.&quot;
        }
    ]
)
print(completion.choices[0].message.content)
</code></pre></div>
<p><strong>With Web Search Options:</strong></p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-python">completion = client.chat.completions.create(
    model=&quot;gpt-4o-search-preview&quot;,
    messages=[
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;What are the latest trends in machine learning?&quot;
        }
    ],
    extra_body={
        &quot;web_search_options&quot;: {
            &quot;enabled&quot;: True,
            &quot;max_results&quot;: 5
        }
    }
)
</code></pre></div>
<h3>Environment Variables</h3>
<p>Reservoir supports customizing provider endpoints via environment variables:</p>
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash"># OpenAI (default: https://api.openai.com/v1/chat/completions)
export RSV_OPENAI_BASE_URL=&quot;https://api.openai.com/v1/chat/completions&quot;

# Ollama (default: http://localhost:11434/v1/chat/completions)
export RSV_OLLAMA_BASE_URL=&quot;http://localhost:11434/v1/chat/completions&quot;

# Mistral (default: https://api.mistral.ai/v1/chat/completions)
export RSV_MISTRAL_BASE_URL=&quot;https://api.mistral.ai/v1/chat/completions&quot;

# API Keys
export OPENAI_API_KEY=&quot;your-openai-key&quot;
export MISTRAL_API_KEY=&quot;your-mistral-key&quot;
export GEMINI_API_KEY=&quot;your-gemini-key&quot;
</code></pre></div>
<h3>Supported Models</h3>
<p>Reservoir automatically routes requests to the appropriate provider based on the model name:</p>
<p>| Model | Provider | API Key Required |
|-------|----------|------------------|
| <code>gpt-4</code>, <code>gpt-4o</code>, <code>gpt-4o-mini</code>, <code>gpt-3.5-turbo</code> | OpenAI | Yes (<code>OPENAI_API_KEY</code>) |
| <code>gpt-4o-search-preview</code> | OpenAI | Yes (<code>OPENAI_API_KEY</code>) |
| <code>llama3.2</code>, <code>gemma3</code>, or any custom name | Ollama | No |
| <code>mistral-large-2402</code> | Mistral | Yes (<code>MISTRAL_API_KEY</code>) |
| <code>gemini-2.0-flash</code>, <code>gemini-2.5-flash-preview-05-20</code> | Google | Yes (<code>GEMINI_API_KEY</code>) |</p>
<p><strong>Note:</strong> Any model name not explicitly configured will default to using Ollama.</p>
<h2>Troubleshooting</h2>
<h3>Common Issues</h3>
<h4>Server Not Starting</h4>
<ul>
<li><strong>Check Neo4j</strong>: Ensure Neo4j is running and accessible</li>
<li><strong>Port Conflicts</strong>: Default port 3017 might be in use. Check with <code>lsof -i :3017</code></li>
<li><strong>Environment</strong>: Source your <code>.envrc</code> if using direnv: <code>direnv allow</code></li>
</ul>
<h4>&quot;Internal Server Error&quot; Responses</h4>
<ul>
<li><strong>API Keys</strong>: Verify your API keys are set correctly:
<div class="code-block"><pre><button class="copy-btn">Copy</button><code class="language-bash">echo $OPENAI_API_KEY
echo $MISTRAL_API_KEY
echo $GEMINI_API_KEY
</code></pre></div>
</li>
<li><strong>Model Names</strong>: Ensure you're using supported model names (see table above)</li>
<li><strong>Ollama</strong>: If using Ollama models, verify Ollama is running: <code>ollama list</code></li>
</ul>
<h4>Deserialization Errors</h4>
<ul>
<li><strong>JSON Format</strong>: Ensure your JSON request is properly formatted</li>
<li><strong>Optional Fields</strong>: Fields like <code>web_search_options</code> are optional and can be omitted</li>
<li><strong>Content-Type</strong>: Always use <code>Content-Type: application/json</code></li>
</ul>
<h4>Connection Issues</h4>
<ul>
<li><strong>Provider URLs</strong>: Check if custom provider URLs are accessible</li>
<li><strong>Network</strong>: Verify internet connectivity for cloud providers</li>
<li><strong>Firewalls</strong>: Ensure no firewall is blocking outbound requests</li>
</ul>
<h4>Request Forwarding</h4>
<ul>
<li><strong>Malformed Requests</strong>: Check server logs for &quot;Failed to parse request body&quot; warnings to see what requests are being forwarded</li>
<li><strong>Missing Features</strong>: Forwarded requests bypass Reservoir's memory and context features</li>
<li><strong>Provider Routing</strong>: Verify the correct API endpoint is being used based on model name</li>
</ul>
<h3>Getting Help</h3>
<p>If you encounter issues:</p>
<ol>
<li>Check the server logs for detailed error messages</li>
<li>Verify your environment variables are set correctly</li>
<li>Test with a simple curl request first</li>
<li>Try the included hurl tests to isolate the problem</li>
</ol>
<h2>License</h2>
<p>This project is licensed under the BSD 3-Clause License - see the <a href="LICENSE">LICENSE</a> file for details.</p>
  </main>

  <footer>
    <hr>
    <p>&copy; 2025 Sector F Labs. All rights reserved.</p>
  </footer>

  <script>
    document.addEventListener('DOMContentLoaded', function() {
      document.querySelectorAll('.copy-btn').forEach(function(btn) {
        btn.addEventListener('click', function(e) {
          e.preventDefault();
          var pre = btn.closest('pre');
          if (!pre) return;
          // Get all code text inside the pre
          var code = pre.querySelector('code');
          var text = code ? code.innerText : pre.innerText;
          navigator.clipboard.writeText(text);
          btn.textContent = 'Copied!';
          setTimeout(function() { btn.textContent = 'Copy'; }, 1200);
        });
      });
    });
  </script>
</body>
</html>
